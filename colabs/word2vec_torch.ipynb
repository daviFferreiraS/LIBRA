{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKOhQuKKRoq6"
      },
      "source": [
        "Implementation of word2vec model based on https://rguigoures.github.io/word2vec_pytorch/ tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3ka0UfWRgUj",
        "outputId": "a0e4a2a7-2e52-4a7e-af10-feda008acb58"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import nltk\n",
        "# nltk.download('brown')\n",
        "# from nltk.corpus import brown\n",
        "import re\n",
        "from numpy.random import multinomial\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JmpZ6kVQS1HV"
      },
      "outputs": [],
      "source": [
        "# corpus = []\n",
        "\n",
        "# # faz-se a tokenização do dataset de noticias:\n",
        "# for cat in [\"news\"]:\n",
        "#   for text_id in brown.fileids(cat):\n",
        "#         raw_text = list(itertools.chain.from_iterable(brown.sents(text_id)))\n",
        "#         text = ' '.join(raw_text)\n",
        "#         text = text.lower()\n",
        "#         text.replace('\\n', ' ')\n",
        "#         text = re.sub('[^a-z ]+', '', text)\n",
        "#         corpus.append([w for w in text.split() if w != ''])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>im updating my blog because i feel shitty</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i never make her separate from me because i do...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i was feeling a little vain when i did this one</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>i cant walk into a shop anywhere where i do no...</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>i felt anger when at the end of a telephone call</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>i explain why i clung to a relationship with a...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>i like to have the same breathless feeling as ...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>i jest i feel grumpy tired and pre menstrual w...</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>i don t feel particularly agitated</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>i feel beautifully emotional knowing that thes...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>i pay attention it deepens into a feeling of b...</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>i just feel extremely comfortable with the gro...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>i find myself in the odd position of feeling s...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>i was feeling as heartbroken as im sure katnis...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>i feel a little mellow today</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>i feel like my only role now would be to tear ...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>i feel just bcoz a fight we get mad to each ot...</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>i feel like reds and purples are just so rich ...</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               phrase  emotion\n",
              "0   im feeling rather rotten so im not very ambiti...  sadness\n",
              "1           im updating my blog because i feel shitty  sadness\n",
              "2   i never make her separate from me because i do...  sadness\n",
              "3   i left with my bouquet of red and yellow tulip...      joy\n",
              "4     i was feeling a little vain when i did this one  sadness\n",
              "5   i cant walk into a shop anywhere where i do no...     fear\n",
              "6    i felt anger when at the end of a telephone call    anger\n",
              "7   i explain why i clung to a relationship with a...      joy\n",
              "8   i like to have the same breathless feeling as ...      joy\n",
              "9   i jest i feel grumpy tired and pre menstrual w...    anger\n",
              "10                 i don t feel particularly agitated     fear\n",
              "11  i feel beautifully emotional knowing that thes...  sadness\n",
              "12  i pay attention it deepens into a feeling of b...     fear\n",
              "13  i just feel extremely comfortable with the gro...      joy\n",
              "14  i find myself in the odd position of feeling s...     love\n",
              "15  i was feeling as heartbroken as im sure katnis...  sadness\n",
              "16                       i feel a little mellow today      joy\n",
              "17  i feel like my only role now would be to tear ...  sadness\n",
              "18  i feel just bcoz a fight we get mad to each ot...    anger\n",
              "19  i feel like reds and purples are just so rich ...      joy"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"dataset copy.csv\", sep=\";\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = []\n",
        "\n",
        "for phrase in df[\"phrase\"]:\n",
        "    corpus.append([word for word in phrase.split() if word != ' '])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['im',\n",
              "  'feeling',\n",
              "  'rather',\n",
              "  'rotten',\n",
              "  'so',\n",
              "  'im',\n",
              "  'not',\n",
              "  'very',\n",
              "  'ambitious',\n",
              "  'right',\n",
              "  'now'],\n",
              " ['im', 'updating', 'my', 'blog', 'because', 'i', 'feel', 'shitty'],\n",
              " ['i',\n",
              "  'never',\n",
              "  'make',\n",
              "  'her',\n",
              "  'separate',\n",
              "  'from',\n",
              "  'me',\n",
              "  'because',\n",
              "  'i',\n",
              "  'don',\n",
              "  't',\n",
              "  'ever',\n",
              "  'want',\n",
              "  'her',\n",
              "  'to',\n",
              "  'feel',\n",
              "  'like',\n",
              "  'i',\n",
              "  'm',\n",
              "  'ashamed',\n",
              "  'with',\n",
              "  'her'],\n",
              " ['i',\n",
              "  'left',\n",
              "  'with',\n",
              "  'my',\n",
              "  'bouquet',\n",
              "  'of',\n",
              "  'red',\n",
              "  'and',\n",
              "  'yellow',\n",
              "  'tulips',\n",
              "  'under',\n",
              "  'my',\n",
              "  'arm',\n",
              "  'feeling',\n",
              "  'slightly',\n",
              "  'more',\n",
              "  'optimistic',\n",
              "  'than',\n",
              "  'when',\n",
              "  'i',\n",
              "  'arrived'],\n",
              " ['i',\n",
              "  'was',\n",
              "  'feeling',\n",
              "  'a',\n",
              "  'little',\n",
              "  'vain',\n",
              "  'when',\n",
              "  'i',\n",
              "  'did',\n",
              "  'this',\n",
              "  'one'],\n",
              " ['i',\n",
              "  'cant',\n",
              "  'walk',\n",
              "  'into',\n",
              "  'a',\n",
              "  'shop',\n",
              "  'anywhere',\n",
              "  'where',\n",
              "  'i',\n",
              "  'do',\n",
              "  'not',\n",
              "  'feel',\n",
              "  'uncomfortable'],\n",
              " ['i',\n",
              "  'felt',\n",
              "  'anger',\n",
              "  'when',\n",
              "  'at',\n",
              "  'the',\n",
              "  'end',\n",
              "  'of',\n",
              "  'a',\n",
              "  'telephone',\n",
              "  'call'],\n",
              " ['i',\n",
              "  'explain',\n",
              "  'why',\n",
              "  'i',\n",
              "  'clung',\n",
              "  'to',\n",
              "  'a',\n",
              "  'relationship',\n",
              "  'with',\n",
              "  'a',\n",
              "  'boy',\n",
              "  'who',\n",
              "  'was',\n",
              "  'in',\n",
              "  'many',\n",
              "  'ways',\n",
              "  'immature',\n",
              "  'and',\n",
              "  'uncommitted',\n",
              "  'despite',\n",
              "  'the',\n",
              "  'excitement',\n",
              "  'i',\n",
              "  'should',\n",
              "  'have',\n",
              "  'been',\n",
              "  'feeling',\n",
              "  'for',\n",
              "  'getting',\n",
              "  'accepted',\n",
              "  'into',\n",
              "  'the',\n",
              "  'masters',\n",
              "  'program',\n",
              "  'at',\n",
              "  'the',\n",
              "  'university',\n",
              "  'of',\n",
              "  'virginia'],\n",
              " ['i',\n",
              "  'like',\n",
              "  'to',\n",
              "  'have',\n",
              "  'the',\n",
              "  'same',\n",
              "  'breathless',\n",
              "  'feeling',\n",
              "  'as',\n",
              "  'a',\n",
              "  'reader',\n",
              "  'eager',\n",
              "  'to',\n",
              "  'see',\n",
              "  'what',\n",
              "  'will',\n",
              "  'happen',\n",
              "  'next'],\n",
              " ['i',\n",
              "  'jest',\n",
              "  'i',\n",
              "  'feel',\n",
              "  'grumpy',\n",
              "  'tired',\n",
              "  'and',\n",
              "  'pre',\n",
              "  'menstrual',\n",
              "  'which',\n",
              "  'i',\n",
              "  'probably',\n",
              "  'am',\n",
              "  'but',\n",
              "  'then',\n",
              "  'again',\n",
              "  'its',\n",
              "  'only',\n",
              "  'been',\n",
              "  'a',\n",
              "  'week',\n",
              "  'and',\n",
              "  'im',\n",
              "  'about',\n",
              "  'as',\n",
              "  'fit',\n",
              "  'as',\n",
              "  'a',\n",
              "  'walrus',\n",
              "  'on',\n",
              "  'vacation',\n",
              "  'for',\n",
              "  'the',\n",
              "  'summer'],\n",
              " ['i', 'don', 't', 'feel', 'particularly', 'agitated'],\n",
              " ['i',\n",
              "  'feel',\n",
              "  'beautifully',\n",
              "  'emotional',\n",
              "  'knowing',\n",
              "  'that',\n",
              "  'these',\n",
              "  'women',\n",
              "  'of',\n",
              "  'whom',\n",
              "  'i',\n",
              "  'knew',\n",
              "  'just',\n",
              "  'a',\n",
              "  'handful',\n",
              "  'were',\n",
              "  'holding',\n",
              "  'me',\n",
              "  'and',\n",
              "  'my',\n",
              "  'baba',\n",
              "  'on',\n",
              "  'our',\n",
              "  'journey'],\n",
              " ['i',\n",
              "  'pay',\n",
              "  'attention',\n",
              "  'it',\n",
              "  'deepens',\n",
              "  'into',\n",
              "  'a',\n",
              "  'feeling',\n",
              "  'of',\n",
              "  'being',\n",
              "  'invaded',\n",
              "  'and',\n",
              "  'helpless'],\n",
              " ['i',\n",
              "  'just',\n",
              "  'feel',\n",
              "  'extremely',\n",
              "  'comfortable',\n",
              "  'with',\n",
              "  'the',\n",
              "  'group',\n",
              "  'of',\n",
              "  'people',\n",
              "  'that',\n",
              "  'i',\n",
              "  'dont',\n",
              "  'even',\n",
              "  'need',\n",
              "  'to',\n",
              "  'hide',\n",
              "  'myself'],\n",
              " ['i',\n",
              "  'find',\n",
              "  'myself',\n",
              "  'in',\n",
              "  'the',\n",
              "  'odd',\n",
              "  'position',\n",
              "  'of',\n",
              "  'feeling',\n",
              "  'supportive',\n",
              "  'of'],\n",
              " ['i',\n",
              "  'was',\n",
              "  'feeling',\n",
              "  'as',\n",
              "  'heartbroken',\n",
              "  'as',\n",
              "  'im',\n",
              "  'sure',\n",
              "  'katniss',\n",
              "  'was'],\n",
              " ['i', 'feel', 'a', 'little', 'mellow', 'today'],\n",
              " ['i',\n",
              "  'feel',\n",
              "  'like',\n",
              "  'my',\n",
              "  'only',\n",
              "  'role',\n",
              "  'now',\n",
              "  'would',\n",
              "  'be',\n",
              "  'to',\n",
              "  'tear',\n",
              "  'your',\n",
              "  'sails',\n",
              "  'with',\n",
              "  'my',\n",
              "  'pessimism',\n",
              "  'and',\n",
              "  'discontent'],\n",
              " ['i',\n",
              "  'feel',\n",
              "  'just',\n",
              "  'bcoz',\n",
              "  'a',\n",
              "  'fight',\n",
              "  'we',\n",
              "  'get',\n",
              "  'mad',\n",
              "  'to',\n",
              "  'each',\n",
              "  'other',\n",
              "  'n',\n",
              "  'u',\n",
              "  'wanna',\n",
              "  'make',\n",
              "  'a',\n",
              "  'publicity',\n",
              "  'n',\n",
              "  'let',\n",
              "  'the',\n",
              "  'world',\n",
              "  'knows',\n",
              "  'about',\n",
              "  'our',\n",
              "  'fight'],\n",
              " ['i',\n",
              "  'feel',\n",
              "  'like',\n",
              "  'reds',\n",
              "  'and',\n",
              "  'purples',\n",
              "  'are',\n",
              "  'just',\n",
              "  'so',\n",
              "  'rich',\n",
              "  'and',\n",
              "  'kind',\n",
              "  'of',\n",
              "  'perfect']]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xdj445kLWMhL"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import random, math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vnPvuAy4p_z"
      },
      "source": [
        "Agora, vamos calcular as ocorrências das palavras no corpus e dps calcular a probabilidade de manter a palavra no corpus, que é definida por:\n",
        "$$P(w_i) = \\frac{10^{-3}}{p_i}(\\sqrt{10^3p_i} + 1)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3HyGJQhK1ofi"
      },
      "outputs": [],
      "source": [
        "def subsample_frequent_words(corpus):\n",
        "  filtered_corpus = []\n",
        "  word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
        "  total = sum(list(word_counts.values()))\n",
        "  # proportion of each word in corpus\n",
        "  word_counts = {word: word_counts[word]/float(total) for word in word_counts}\n",
        "\n",
        "  for text in corpus:\n",
        "    filtered_corpus.append([])\n",
        "    for word in text:\n",
        "      if random.random() < (1+math.sqrt(word_counts[word]*1e3) * 1e-3 / word_counts[word]):\n",
        "        filtered_corpus[-1].append(word)\n",
        "  return filtered_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IlRrOX7o2RIb"
      },
      "outputs": [],
      "source": [
        "corpus = subsample_frequent_words(corpus)\n",
        "vocabulary = set(itertools.chain.from_iterable(corpus))\n",
        "\n",
        "word_to_index = {word: index for (index, word) in enumerate(vocabulary)}\n",
        "index_to_word = {index: word for (index, word) in enumerate(vocabulary)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG1ntn5B-6Xc"
      },
      "source": [
        "Agora construindo o Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aMNScpNIFOcC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8pgTfzlq2Bmi"
      },
      "outputs": [],
      "source": [
        "def get_batches(context, batch_size = 100):\n",
        "  random.shuffle(context)\n",
        "  batches = []\n",
        "  batch_target, batch_meaning= [], []\n",
        "  for i in range(len(context)):\n",
        "\n",
        "    batch_target.append(word_to_index[context[i][0]])\n",
        "    batch_meaning.append(word_to_index[context[i][1]])\n",
        "\n",
        "    if (i+1) % batch_size or i == len(context) - 1:\n",
        "      tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
        "      tensor_meaning = autograd.Variable(torch.from_numpy(np.array(batch_meaning)).long())\n",
        "      batches.append((tensor_target, tensor_meaning))\n",
        "      batch_target, batch_meaning = [], []\n",
        "  return batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3Y_Z9MoPW1oC"
      },
      "outputs": [],
      "source": [
        "def sample_negative(size):\n",
        "  probability = {}\n",
        "  word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
        "  normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
        "\n",
        "  for word in word_counts:\n",
        "    probability[word] = word_counts[word]**0.75 / normalizing_factor\n",
        "\n",
        "  words = np.array(list(word_counts.keys()))\n",
        "\n",
        "  while True:\n",
        "    word_list = []\n",
        "    index = np.array(multinomial(size, list(probability.values())))\n",
        "    for i, count in enumerate(index):\n",
        "      for _ in range(count):\n",
        "        word_list.append(words[i])\n",
        "    yield word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fEpgCHbnCMnO"
      },
      "outputs": [],
      "source": [
        "context = []\n",
        "window = 4\n",
        "# negative_samples = sample_negative(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dar uma olhada no nest_asyncio para paralelização do bloco abaixo e se possível melhorar o resto do programa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "J8aIYesOBI3U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tem-se um total de 2018 pares de palavras alvo e palavras contextuais\n"
          ]
        }
      ],
      "source": [
        "for text in corpus:\n",
        "  for i, word in enumerate(text):\n",
        "    start = max(0, i-window)\n",
        "    end = min(i+window, len(text))\n",
        "    for j in range(start, end):\n",
        "      if i != j:\n",
        "        context.append((word,text[j]))\n",
        "\n",
        "print(f\"Tem-se um total de {len(context)} pares de palavras alvo e palavras contextuais\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxHnZ57-FRQf"
      },
      "source": [
        "# Criando de fato o word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t81YdE-7JfoE"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, embedding_size, vocab_size):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "    self.context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "  def forward(self, target_word, context_word):\n",
        "    emb_target = self.target_embeddings(target_word)\n",
        "    emb_context = self.context_embeddings(context_word)\n",
        "    emb_product = torch.mul(emb_target, emb_context)\n",
        "    emb_product = torch.sum(emb_product, dim=1)\n",
        "    output = torch.sum(F.logsigmoid(emb_product))\n",
        "    # emb_negative = self.context_embeddings(negative_example)\n",
        "    # emb_product = torch.bmm(emb_negative, emb_target.unsqueeze(2))\n",
        "    # emb_product = torch.sum(emb_product, dim=1)\n",
        "    # output += torch.sum(F.logsigmoid(-emb_product))\n",
        "    return -output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9FwLg5uxSiC"
      },
      "source": [
        "Definição de uma função que para o aprendizado assim que a função de custo parar de decrescer significantemente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YDDZ3gH4xNfT"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping():\n",
        "  def __init__(self, patience=5, min_percent_gain=1):\n",
        "    self.patience = patience\n",
        "    self.loss = []\n",
        "    self.min_percent_gain = min_percent_gain/100.\n",
        "\n",
        "  def update_loss(self, loss):\n",
        "    self.loss.append(loss)\n",
        "    if len(self.loss) > self.patience:\n",
        "      del self.loss[0]\n",
        "\n",
        "  def stop_training(self):\n",
        "    if len(self.loss) == 1:\n",
        "      return False\n",
        "\n",
        "    gain = (max(self.loss) - min(self.loss)/max(self.loss))\n",
        "    print(f\"Ganho de custo: {round(100*gain,2)}\")\n",
        "    return gain < self.min_percent_gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnszTpiO15Lz"
      },
      "source": [
        "Etapa de aprendizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TRP9DVIf14kU"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocabulary)\n",
        "\n",
        "w2v = Word2Vec(embedding_size= 200, vocab_size = vocab_size)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(w2v.parameters())\n",
        "early_stopping = EarlyStopping()\n",
        "context_tensor = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UX8GMt58t7g2"
      },
      "outputs": [],
      "source": [
        "for target, meaning in context:\n",
        "  target_tensor = autograd.Variable(torch.LongTensor([word_to_index[target]]))\n",
        "  meaning_tensor = autograd.Variable(torch.LongTensor([word_to_index[meaning]]))\n",
        "  context_tensor.append((target_tensor, meaning_tensor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MtsRx2kvo9d",
        "outputId": "bb0b4fa9-8200-495d-93d2-e77de220874b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss:  5.37983\n",
            "loss:  3.1180847\n",
            "Ganho de custo: 480.02\n",
            "loss:  1.5449657\n",
            "Ganho de custo: 509.27\n",
            "loss:  0.66552913\n",
            "Ganho de custo: 525.61\n",
            "loss:  0.25723094\n",
            "Ganho de custo: 533.2\n",
            "loss:  0.078618094\n",
            "Ganho de custo: 309.29\n",
            "loss:  0.016831018\n",
            "Ganho de custo: 153.41\n",
            "loss:  0.0041913274\n",
            "Ganho de custo: 65.92\n",
            "loss:  0.0005354312\n",
            "Ganho de custo: 25.51\n",
            "loss:  0.00022546829\n",
            "Ganho de custo: 7.58\n",
            "loss:  8.80563e-05\n",
            "Ganho de custo: 1.16\n",
            "loss:  3.4798308e-05\n",
            "Ganho de custo: -0.41\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  losses = []\n",
        "  context_batches = get_batches(context= context, batch_size=1000)\n",
        "  for i in range(len(context_batches)):\n",
        "    w2v.zero_grad()\n",
        "    target_tensor, meaning_tensor = context_batches[i]\n",
        "    loss = w2v(target_tensor, meaning_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.data)\n",
        "  print(\"loss: \", np.mean(losses))\n",
        "  early_stopping.update_loss(np.mean(losses))\n",
        "  if early_stopping.stop_training():\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "testar com um dataset pequeno (20 coisas), com o sample de negativos e sem"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
